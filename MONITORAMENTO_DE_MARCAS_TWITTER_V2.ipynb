{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 170
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 7101,
     "status": "ok",
     "timestamp": 1572803534352,
     "user": {
      "displayName": "Gustavo Morozi",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mByW0rsjc2JLOgtMEJbLYI-wp-fX33YcI07IA3b2g=s64",
      "userId": "12380855071894952955"
     },
     "user_tz": 120
    },
    "id": "3_ALg-Uc5dvZ",
    "outputId": "651bc27e-3023-4b5f-98f3-87e402acfe0a"
   },
   "outputs": [],
   "source": [
    "# Importando as bibliotecas \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import twitter\n",
    "import json\n",
    "import nltk\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "%matplotlib inline\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_twitter = pd.read_csv(\"BaseTwitter -ORIGINAL.csv\",sep=\",\",encoding=\"ISO-8859-1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_treinamento, base_teste=train_test_split(df_twitter)  #75% para treino e 25% para teste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tamanho da base de Treinamento 2998\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "alegria     1560\n",
       "tristeza    1438\n",
       "Name: Sentimento, dtype: int64"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Transforma base em pandas\n",
    "Analise_Treinamento=pd.DataFrame(base_treinamento)\n",
    "\n",
    "#Dá nome as colunas\n",
    "Analise_Treinamento.columns=['Frase','Sentimento']\n",
    "\n",
    "# Mostra tamanho da base de treinamento\n",
    "print('Tamanho da base de Treinamento {}'.format(Analise_Treinamento.shape[0]))\n",
    "\n",
    "# Mostra quantidade de cada tipo de sentimento\n",
    "Analise_Treinamento.Sentimento.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alegria     52.03469\n",
      "tristeza    47.96531\n",
      "Name: Sentimento, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Percentual de cada tipo de sentimento\n",
    "print((Analise_Treinamento.Sentimento.value_counts()/exemplo_base.shape[0])*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tamanho da base de Teste 1000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "alegria     526\n",
       "tristeza    474\n",
       "Name: Sentimento, dtype: int64"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Criar base teste\n",
    "# Transforma base em pandas\n",
    "Analise_Teste=pd.DataFrame(base_teste)\n",
    "\n",
    "#Dá nome as colunas\n",
    "Analise_Teste.columns=['Frase','Sentimento']\n",
    "\n",
    "# Mostra tamanho da base de treinamento\n",
    "print('Tamanho da base de Teste {}'.format(Analise_Teste.shape[0]))\n",
    "\n",
    "# Mostra quantidade de cada tipo de sentimento\n",
    "Analise_Teste.Sentimento.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['de', 'a', 'o', 'que', 'e', 'é', 'do', 'da', 'em', 'um', 'para',\n",
       "       'com', 'não', 'uma', 'os', 'no', 'se', 'na', 'por', 'mais', 'as',\n",
       "       'dos', 'como', 'mas', 'ao', 'ele', 'das', 'à', 'seu', 'sua', 'ou',\n",
       "       'quando', 'muito', 'nos', 'já', 'eu', 'também', 'só', 'pelo',\n",
       "       'pela', 'até', 'isso', 'ela', 'entre', 'depois', 'sem', 'mesmo',\n",
       "       'aos', 'seus', 'quem', 'nas', 'me', 'esse', 'eles', 'você', 'essa',\n",
       "       'num', 'nem', 'suas', 'meu', 'às', 'minha', 'numa', 'pelos',\n",
       "       'elas', 'qual', 'nós', 'lhe', 'deles', 'essas', 'esses', 'pelas',\n",
       "       'este', 'dele', 'tu', 'te', 'vocês', 'vos', 'lhes', 'meus',\n",
       "       'minhas', 'teu', 'tua', 'teus', 'tuas', 'nosso', 'nossa', 'nossos',\n",
       "       'nossas', 'dela', 'delas', 'esta', 'estes', 'estas', 'aquele',\n",
       "       'aquela', 'aqueles', 'aquelas', 'isto', 'aquilo', 'estou', 'está',\n",
       "       'estamos', 'estão', 'estive', 'esteve', 'estivemos', 'estiveram',\n",
       "       'estava', 'estávamos', 'estavam', 'estivera', 'estivéramos',\n",
       "       'esteja', 'estejamos', 'estejam', 'estivesse', 'estivéssemos',\n",
       "       'estivessem', 'estiver', 'estivermos', 'estiverem', 'hei', 'há',\n",
       "       'havemos', 'hão', 'houve', 'houvemos', 'houveram', 'houvera',\n",
       "       'houvéramos', 'haja', 'hajamos', 'hajam', 'houvesse',\n",
       "       'houvéssemos', 'houvessem', 'houver', 'houvermos', 'houverem',\n",
       "       'houverei', 'houverá', 'houveremos', 'houverão', 'houveria',\n",
       "       'houveríamos', 'houveriam', 'sou', 'somos', 'são', 'era', 'éramos',\n",
       "       'eram', 'fui', 'foi', 'fomos', 'foram', 'fora', 'fôramos', 'seja',\n",
       "       'sejamos', 'sejam', 'fosse', 'fôssemos', 'fossem', 'for', 'formos',\n",
       "       'forem', 'serei', 'será', 'seremos', 'serão', 'seria', 'seríamos',\n",
       "       'seriam', 'tenho', 'tem', 'temos', 'tém', 'tinha', 'tínhamos',\n",
       "       'tinham', 'tive', 'teve', 'tivemos', 'tiveram', 'tivera',\n",
       "       'tivéramos', 'tenha', 'tenhamos', 'tenham', 'tivesse',\n",
       "       'tivéssemos', 'tivessem', 'tiver', 'tivermos', 'tiverem', 'terei',\n",
       "       'terá', 'teremos', 'terão', 'teria', 'teríamos', 'teriam'],\n",
       "      dtype='<U12')"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#nltk.download('stopwords')\n",
    "lista_Stop=nltk.corpus.stopwords.words('portuguese')\n",
    "np.transpose(lista_Stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função que retira StopWords\n",
    "def RemoveStopWords(texto):\n",
    "    frases=[]\n",
    "    for (palavras, sentimento) in texto:\n",
    "        #Criamos uma list compreheension para extrair apenas as palavras que não estão na list_stop\n",
    "        semStop= [ p for p in palavras.split() if p not in lista_Stop]\n",
    "        #Inserindo as frases com os Labels (sentimento) ja tatadas pela Lista_Stop\n",
    "        frases.append((semStop,sentimento))\n",
    "    return frases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Steemer= técnica de remover sufixos e prefixos de uma palavra,Por exemplo, o stem da palavra cooking é cook. Um bom algoritmo sabe que “ing” é um sufixo e pode ser removido.\n",
    "#nltk.download('rslp')\n",
    "def aplica_Stemmer(texto):\n",
    "    stemmer=nltk.stem.RSLPStemmer()\n",
    "    #Escolhido o RSLPS pois é especifico da língua portuguesas\n",
    "    frases_sem_Stemming=[]\n",
    "    for (palavras, sentimento) in texto:\n",
    "        com_Stemming=[str(stemmer.stem(p)) for p in palavras.split() if p not in lista_Stop]\n",
    "        frases_sem_Stemming.append((com_Stemming,sentimento))\n",
    "    return frases_sem_Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Transforma base de treinamento em um array\n",
    "base_treinamento=np.array(base_treinamento)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aplica o Stemer na Frase\n",
    "# Transformar em vetor está dando problema\n",
    "frases_com_Stem_treinamento=aplica_Stemmer(base_treinamento)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Transforma base de teste em um array\n",
    "base_teste=np.array(base_teste)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aplica Stem na base de teste\n",
    "frases_com_Stem_teste=aplica_Stemmer(base_teste)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cria função para retornar as palavras da frase, sem a classificação(sentimento)\n",
    "def busca_Palavras(frases):\n",
    "    todas_Palavras=[]\n",
    "    for (palavras,sentimento) in frases:\n",
    "        todas_Palavras.extend(palavras)\n",
    "    return todas_Palavras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aplica função acima em cada base\n",
    "palavras_treinamento=busca_Palavras(frases_com_Stem_treinamento)\n",
    "palavras_teste=busca_Palavras(frases_com_Stem_teste)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quantidade de palavras Treinamento 0    37105\n",
      "dtype: int64\n",
      "Quantidade de palavras Teste 0    12859\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Mostra quantidade de palavras na base\n",
    "print (\"Quantidade de palavras Treinamento {}\".format(pd.DataFrame(palavras_treinamento).count()))\n",
    "print (\"Quantidade de palavras Teste {}\".format(pd.DataFrame(palavras_teste).count()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função para verificar a quantidade de vezes que a palavras é mencionada\n",
    "def busca_frequencia(palavras):\n",
    "    palavras=nltk.FreqDist(palavras)\n",
    "    return palavras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chama função acima\n",
    "frequencia_treinamento=busca_frequencia(palavras_treinamento)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('iphon', 2964),\n",
       " ('compr', 541),\n",
       " ('quebr', 497),\n",
       " ('ganh', 459),\n",
       " ('estrag', 417),\n",
       " ('nã£', 409),\n",
       " ('troq', 409),\n",
       " ('molh', 363),\n",
       " ('q', 335),\n",
       " ('pra', 329),\n",
       " ('consert', 320),\n",
       " ('meu', 290),\n",
       " ('celul', 289),\n",
       " ('caiu', 287),\n",
       " ('eu', 249),\n",
       " ('iphone,', 237),\n",
       " ('ã©', 224),\n",
       " (',', 185),\n",
       " ('to', 178),\n",
       " ('tel', 175)]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# A função most_common do NLTK possibilita visualizar quais as palavras que ocorrem com maior frequência em nosso texto.\n",
    "frequencia_treinamento.most_common(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Executa função parecida com a mostcommon\n",
    "frequencia_teste=busca_frequencia(palavras_teste)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função para retornar somente as palavras unicas\n",
    "def busca_palavras_unicas(frequencia):\n",
    "    freq=frequencia.keys()\n",
    "    return freq\n",
    "\n",
    "# Chama função acima\n",
    "palavras_unicas_treinamento=busca_palavras_unicas(frequencia_treinamento)\n",
    "palavras_unicas_teste=busca_palavras_unicas(frequencia_teste)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cria função para identificar quais as palavras únicas estão no docuemnto passado para a função\n",
    "def extrator_palavras(documento):\n",
    "        #Utilizado set() para associar a variável doc.com o parametro que esta chegando\n",
    "        doc=set(documento)\n",
    "        caracteristicas={}\n",
    "        for palavras in palavras_unicas_treinamento:\n",
    "            caracteristicas['%s' % palavras]=(palavras in doc)\n",
    "        return caracteristicas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# devido a necessidade de aplciação da função Extrato_Palavras sobre as bases de Treinamento e Teste, como a variável palavras_unicas_teste exige aplicação isolada, precisamos criar uma função apartada somente\n",
    "# para a base de teste\n",
    "def extrator_palavras_teste(documento):\n",
    "    doc=set(documento)\n",
    "    caracteristicas={}\n",
    "    for palavras in palavras_unicas_teste:\n",
    "        caracteristicas['%s' % palavras]=(palavras in doc)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_completa_treinamento=nltk.classify.apply_features(extrator_palavras, frases_com_Stem_treinamento)\n",
    "base_completa_teste=nltk.classify.apply_features(extrator_palavras_teste,frases_com_Stem_teste)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['tristeza', 'alegria']\n"
     ]
    }
   ],
   "source": [
    "# O algoritmo NaiveBayes monta a tabela de probabilidade\n",
    "classificador=nltk.NaiveBayesClassifier.train(base_completa_treinamento)\n",
    "print(classificador.labels())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Informative Features\n",
      "                    caiu = True           triste : alegri =     63.5 : 1.0\n",
      "                  estrag = True           triste : alegri =     48.4 : 1.0\n",
      "                       @ = True           triste : alegri =     41.0 : 1.0\n",
      "                       . = True           triste : alegri =     30.7 : 1.0\n",
      "                  derrub = True           triste : alegri =     30.0 : 1.0\n",
      "                     ess = True           triste : alegri =     25.7 : 1.0\n",
      "                 consert = True           alegri : triste =     20.2 : 1.0\n",
      "                    dinh = True           alegri : triste =     16.9 : 1.0\n",
      "                    troq = True           alegri : triste =     14.3 : 1.0\n",
      "                     ... = True           triste : alegri =     12.5 : 1.0\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(classificador.show_most_informative_features(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(None, 'alegria'), (None, 'tristeza'), ...]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Verifica a acuracia do modelo\n",
    "print(nltk.classify.accuracy(classificador,base_completa_teste))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'copy'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-64-26219f84e62c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[1;31m#print(frase)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[1;31m#print(classe)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m     \u001b[0mresultado\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mclassificador\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclassify\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfrase\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mresultado\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mclasse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m         \u001b[0merros\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclasse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mresultado\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mfrase\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\nltk\\classify\\naivebayes.py\u001b[0m in \u001b[0;36mclassify\u001b[1;34m(self, featureset)\u001b[0m\n\u001b[0;32m     88\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     89\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclassify\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeatureset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 90\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprob_classify\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeatureset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     91\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     92\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mprob_classify\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeatureset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\nltk\\classify\\naivebayes.py\u001b[0m in \u001b[0;36mprob_classify\u001b[1;34m(self, featureset)\u001b[0m\n\u001b[0;32m     94\u001b[0m         \u001b[1;31m# Otherwise, we'll just assign a probability of 0 to\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     95\u001b[0m         \u001b[1;31m# everything.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 96\u001b[1;33m         \u001b[0mfeatureset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfeatureset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     97\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mfname\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeatureset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     98\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mlabel\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_labels\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'copy'"
     ]
    }
   ],
   "source": [
    "# Agrega linhas com erros de classificação\n",
    "erros=[]\n",
    "for (frase,classe) in base_completa_teste:\n",
    "    #print(frase)\n",
    "    #print(classe)\n",
    "    resultado=classificador.classify(frase)\n",
    "    if resultado != classe:\n",
    "        erros.append((classe,resultado,frase))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'copy'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-72-27f19c7d9902>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mprevisto\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mfrase\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mclasse\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mbase_completa_teste\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m     \u001b[0mresultado\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mclassificador\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclassify\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfrase\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m     \u001b[0mprevisto\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresultado\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[0mesperando\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclasse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\nltk\\classify\\naivebayes.py\u001b[0m in \u001b[0;36mclassify\u001b[1;34m(self, featureset)\u001b[0m\n\u001b[0;32m     88\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     89\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclassify\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeatureset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 90\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprob_classify\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeatureset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     91\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     92\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mprob_classify\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeatureset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\nltk\\classify\\naivebayes.py\u001b[0m in \u001b[0;36mprob_classify\u001b[1;34m(self, featureset)\u001b[0m\n\u001b[0;32m     94\u001b[0m         \u001b[1;31m# Otherwise, we'll just assign a probability of 0 to\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     95\u001b[0m         \u001b[1;31m# everything.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 96\u001b[1;33m         \u001b[0mfeatureset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfeatureset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     97\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mfname\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeatureset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     98\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mlabel\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_labels\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'copy'"
     ]
    }
   ],
   "source": [
    "# Cria matriz de confusão\n",
    "from nltk.metrics import ConfusionMatrix\n",
    "esperado=[]\n",
    "previsto=[]\n",
    "for (frase,classe) in base_completa_teste:\n",
    "    resultado=classificador.classify(frase)\n",
    "    previsto.append(resultado)\n",
    "    esperando.append(classe)\n",
    "\n",
    "matriz=ConfusionMatrix(esperado,previsto)\n",
    "print (matriz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tristeza: 0.240523\n",
      "alegria: 0.759477\n"
     ]
    }
   ],
   "source": [
    "teste=\"Comprei um celular novo!\"\n",
    "testeStemming=[]\n",
    "stemmer=nltk.stem.RSLPStemmer()\n",
    "for (palavras_treinamento) in teste.split():\n",
    "    comStem=[p for p in palavras_treinamento.split()]\n",
    "    testeStemming.append(str(stemmer.stem(comStem[0])))\n",
    "\n",
    "novo=extrator_palavras(testeStemming)\n",
    "#print(classificador.classify(novo))\n",
    "distribuicao = classificador.prob_classify(novo)\n",
    "for classe in distribuicao.samples():\n",
    "    print('%s: %f' % (classe, distribuicao.prob(classe)))\n",
    "    \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "LendoTwitter.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
