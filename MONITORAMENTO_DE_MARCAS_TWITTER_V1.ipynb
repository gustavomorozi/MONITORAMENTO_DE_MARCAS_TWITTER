{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 170
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 7101,
     "status": "ok",
     "timestamp": 1572803534352,
     "user": {
      "displayName": "Gustavo Morozi",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mByW0rsjc2JLOgtMEJbLYI-wp-fX33YcI07IA3b2g=s64",
      "userId": "12380855071894952955"
     },
     "user_tz": 120
    },
    "id": "3_ALg-Uc5dvZ",
    "outputId": "651bc27e-3023-4b5f-98f3-87e402acfe0a"
   },
   "outputs": [],
   "source": [
    "# Importando as bibliotecas \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import twitter\n",
    "import json\n",
    "import nltk\n",
    "\n",
    "%matplotlib inline\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 724,
     "status": "ok",
     "timestamp": 1572803572268,
     "user": {
      "displayName": "Gustavo Morozi",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mByW0rsjc2JLOgtMEJbLYI-wp-fX33YcI07IA3b2g=s64",
      "userId": "12380855071894952955"
     },
     "user_tz": 120
    },
    "id": "YJZ75QuH5dvf",
    "outputId": "35425a5b-c893-4424-b780-1b40cfe6c1e0",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"created_at\": \"Thu Apr 04 00:04:42 +0000 2013\", \"default_profile\": true, \"favourites_count\": 38, \"followers_count\": 108, \"friends_count\": 1335, \"id\": 1325712606, \"id_str\": \"1325712606\", \"listed_count\": 81, \"name\": \"Gustavo Morozi\", \"profile_background_color\": \"C0DEED\", \"profile_background_image_url\": \"http://abs.twimg.com/images/themes/theme1/bg.png\", \"profile_background_image_url_https\": \"https://abs.twimg.com/images/themes/theme1/bg.png\", \"profile_banner_url\": \"https://pbs.twimg.com/profile_banners/1325712606/1482203973\", \"profile_image_url\": \"http://pbs.twimg.com/profile_images/811048272970338308/Tb1Axe5E_normal.jpg\", \"profile_image_url_https\": \"https://pbs.twimg.com/profile_images/811048272970338308/Tb1Axe5E_normal.jpg\", \"profile_link_color\": \"1DA1F2\", \"profile_sidebar_border_color\": \"C0DEED\", \"profile_sidebar_fill_color\": \"DDEEF6\", \"profile_text_color\": \"333333\", \"profile_use_background_image\": true, \"screen_name\": \"gustavomorozi\", \"status\": {\"created_at\": \"Tue May 09 01:28:32 +0000 2017\", \"id\": 861754728920866816, \"id_str\": \"861754728920866816\", \"lang\": \"pt\", \"source\": \"<a href=\\\"https://mobile.twitter.com\\\" rel=\\\"nofollow\\\">Twitter Web App</a>\", \"text\": \"Eu falei pra voc\\u00ea ir mas no fundo eu queria que voc\\u00ea n\\u00e3o tivesse me escutado, queria voc\\u00ea tivesse ficado.\"}, \"statuses_count\": 80}\n"
     ]
    }
   ],
   "source": [
    "#Carrega Arquivo de configura√ß√£o\n",
    "with open('tokens.json') as f:\n",
    "    data = json.load(f)\n",
    "    \n",
    "#Carrega a API do Twitter, utilizando os dados do arquivo json:\n",
    "api = twitter.Api(**data)\n",
    "\n",
    "# Printa a credencial\n",
    "print(api.VerifyCredentials())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6jpkC8Ty5dvr"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>User</th>\n",
       "      <th>Tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>jjonkies</td>\n",
       "      <td>Pior que quando comprei meu iphone 11 tinha da...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>eufelipemattos</td>\n",
       "      <td>comprei outro iPhone</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sccpgi_</td>\n",
       "      <td>esse povo s√≥ me ama agr pq comprei um iphone \\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>landinlar</td>\n",
       "      <td>vcs acreditam nessas coisas \"comprei um iphone...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Giovann42832153</td>\n",
       "      <td>eu sonhei que comprei um IPhone X, Deus, se fo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>_laryyleal</td>\n",
       "      <td>N√£o sei onde eu estava c a cabe√ßa quando compr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>CheneneJ</td>\n",
       "      <td>@Pooh68640704 Really bby, comprei meu iPhone 7...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>lokiIang</td>\n",
       "      <td>peguei a parte q j√° tinha da festa e comprei u...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>quezonaa</td>\n",
       "      <td>Desde quando eu comprei o meu iPhone eu nunca ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>LucianoPhilip2</td>\n",
       "      <td>Eu n√£o tirava foto porque achava que era a c√¢m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>LReixs</td>\n",
       "      <td>Eu quando comprei meu iPhone: https://t.co/7cf...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>wjvergetti</td>\n",
       "      <td>Preciso de um IPhone novo....\\nSaudades de qua...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>pauliinbfr</td>\n",
       "      <td>@bxotafogo comprei um A70, ent√£o vou sortear m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>davi_pussatelli</td>\n",
       "      <td>E do nada bateu a loucura de compra um iPhone ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>camisfm</td>\n",
       "      <td>Guinho n√£o mentiu. Eu mijo com gente que acha ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>kakaumartins_</td>\n",
       "      <td>comprei meu J7 prime no come√ßo de 2018 e at√© h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>brunaa_luiza3</td>\n",
       "      <td>comprei meu iPhone, to feliz ü•∞</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>principeedeouro</td>\n",
       "      <td>Comprei um carregador pro meu Iphone ontem mas...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>rafaellagomesx</td>\n",
       "      <td>Fazendo varias compras aq com o cart√£o do @and...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>duzinho_lp</td>\n",
       "      <td>comprei meu iphone XR finalmente</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>marciocrvg1999</td>\n",
       "      <td>Comprei um cabo do iphone, s√≥ paguei o frete 4...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>_vallegabriela</td>\n",
       "      <td>Comprei um iPhone e t√¥ levando mais de uma hor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>roberiott</td>\n",
       "      <td>Gente comprei um iPhone XR me sentindo feliz</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Claudio_RD350LC</td>\n",
       "      <td>@mgjosejesus @TheoDAlmeida2 @malungorei @Brazi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>MorgaoFreud</td>\n",
       "      <td>man comprei o chromecast 3 hj lembrei que ia p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>anttxsanahi</td>\n",
       "      <td>Comprei sim Fundas novas iPhone 11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>anttxsanahi</td>\n",
       "      <td>comprei umas capinhas iphone 11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Criistianleo</td>\n",
       "      <td>Comprei outro iPhone, tentei n√£o consigo mais ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>ericarrache</td>\n",
       "      <td>@ViuvasDoMame Eu comprei um GPD XD (2 na verda...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>Gabe_barboza</td>\n",
       "      <td>Sempre desejei um iPhone, comprei fodase</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>Biiancabecker</td>\n",
       "      <td>Comprei o iPhone 7 Plus, aulas demais ü§§</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>TuitaJessica</td>\n",
       "      <td>Comprei um fone de ouvido igual o do iPhone da...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>LiloOuchOficial</td>\n",
       "      <td>Eu comprei um iphone 11 pro meu namorado. Pqp ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>nathsfernandes_</td>\n",
       "      <td>n√£o comprei outro celular at√© agora aguardando...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>SpinaBela</td>\n",
       "      <td>Gente do ceu! \\nEu passei a semana mais agunia...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>badgalguiga</td>\n",
       "      <td>‚ÄúNao me diga q isso √© o iphone 11‚Äù\\n-Nao, comp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>capitaoluca</td>\n",
       "      <td>Quem dera se o fato de eu ter um iPhone me fiz...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>eulorenaa</td>\n",
       "      <td>Comprei meu iPhone na segunda-feira e minha m√£...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>Quaxlo</td>\n",
       "      <td>fracassado o q  pow comprei um iphone a vista ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>LoohMartins_19</td>\n",
       "      <td>Meu irm√£o: Comprei o iPhone 7 plus \\nEu: pra m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>gvsilva7</td>\n",
       "      <td>Comprei um iPhone pra minha princesa üòù‚ò∫Ô∏è</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>rafah_avila04</td>\n",
       "      <td>Comprei um iphone dnv ! Desculpa xiaome ! Foi ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>AlineZdziarski</td>\n",
       "      <td>vcs precisam PARAR de tentar diminuir os outro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>liipw12</td>\n",
       "      <td>Eu comprei iPhone 5s to  gostando bastante mas...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>EdiCostta</td>\n",
       "      <td>@Tony_Alive1 iPhone 11, comprado em Orlando se...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>amandda_1</td>\n",
       "      <td>Batia o p√© falando que eu s√≥ comprava celular ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>KarolBarcello12</td>\n",
       "      <td>Comprei o iPhone 8 Plus, mas j√° quero o 11 pro ü§≠</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>exoblwck</td>\n",
       "      <td>vai fazer 2 meses q eu comprei um iphone e at√©...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>gabynbarcelos</td>\n",
       "      <td>Mal comprei um cabo Beja deu ruim, pq iPhone ....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>johnnywendel03</td>\n",
       "      <td>Comprei hoje um iPhone pra minha prima.. 11 an...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>luizaa_tk</td>\n",
       "      <td>@Elizaa_coser j√° comprei meu iphone novo amiga...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>guxxtavosantana</td>\n",
       "      <td>Quase comprei um iphone 7, mas no fim peguei o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>viihs2z</td>\n",
       "      <td>Comprei um iPhone 11 am√©m üôèüèºü•∞</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>clyntonribeiroo</td>\n",
       "      <td>Comprei um iPhone e n sei mexer üò≠</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>JackCrvlh26</td>\n",
       "      <td>Eu me estresso com IPhone n sei pra que compre...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>fofasuxpiro</td>\n",
       "      <td>@thesasar4 entao sara eu vendi meu rim esquerd...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>wellslv_</td>\n",
       "      <td>@focusrare Eu quando comprei meu iphone</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>hannaclarahs</td>\n",
       "      <td>comprei um iPhone 7 e n√£o sei usar, continuo u...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>MicheleLeone0</td>\n",
       "      <td>Comprei 10 capas pro meu iPhone</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>danigibin</td>\n",
       "      <td>@Apple comprei meu iPhone 7 que uso atualmente...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows √ó 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               User                                              Tweet\n",
       "0          jjonkies  Pior que quando comprei meu iphone 11 tinha da...\n",
       "1    eufelipemattos                               comprei outro iPhone\n",
       "2           sccpgi_  esse povo s√≥ me ama agr pq comprei um iphone \\...\n",
       "3         landinlar  vcs acreditam nessas coisas \"comprei um iphone...\n",
       "4   Giovann42832153  eu sonhei que comprei um IPhone X, Deus, se fo...\n",
       "5        _laryyleal  N√£o sei onde eu estava c a cabe√ßa quando compr...\n",
       "6          CheneneJ  @Pooh68640704 Really bby, comprei meu iPhone 7...\n",
       "7          lokiIang  peguei a parte q j√° tinha da festa e comprei u...\n",
       "8          quezonaa  Desde quando eu comprei o meu iPhone eu nunca ...\n",
       "9    LucianoPhilip2  Eu n√£o tirava foto porque achava que era a c√¢m...\n",
       "10           LReixs  Eu quando comprei meu iPhone: https://t.co/7cf...\n",
       "11       wjvergetti  Preciso de um IPhone novo....\\nSaudades de qua...\n",
       "12       pauliinbfr  @bxotafogo comprei um A70, ent√£o vou sortear m...\n",
       "13  davi_pussatelli  E do nada bateu a loucura de compra um iPhone ...\n",
       "14          camisfm  Guinho n√£o mentiu. Eu mijo com gente que acha ...\n",
       "15    kakaumartins_  comprei meu J7 prime no come√ßo de 2018 e at√© h...\n",
       "16    brunaa_luiza3                     comprei meu iPhone, to feliz ü•∞\n",
       "17  principeedeouro  Comprei um carregador pro meu Iphone ontem mas...\n",
       "18   rafaellagomesx  Fazendo varias compras aq com o cart√£o do @and...\n",
       "19       duzinho_lp                   comprei meu iphone XR finalmente\n",
       "20   marciocrvg1999  Comprei um cabo do iphone, s√≥ paguei o frete 4...\n",
       "21   _vallegabriela  Comprei um iPhone e t√¥ levando mais de uma hor...\n",
       "22        roberiott       Gente comprei um iPhone XR me sentindo feliz\n",
       "23  Claudio_RD350LC  @mgjosejesus @TheoDAlmeida2 @malungorei @Brazi...\n",
       "24      MorgaoFreud  man comprei o chromecast 3 hj lembrei que ia p...\n",
       "25      anttxsanahi                 Comprei sim Fundas novas iPhone 11\n",
       "26      anttxsanahi                    comprei umas capinhas iphone 11\n",
       "27     Criistianleo  Comprei outro iPhone, tentei n√£o consigo mais ...\n",
       "28      ericarrache  @ViuvasDoMame Eu comprei um GPD XD (2 na verda...\n",
       "29     Gabe_barboza           Sempre desejei um iPhone, comprei fodase\n",
       "..              ...                                                ...\n",
       "70    Biiancabecker            Comprei o iPhone 7 Plus, aulas demais ü§§\n",
       "71     TuitaJessica  Comprei um fone de ouvido igual o do iPhone da...\n",
       "72  LiloOuchOficial  Eu comprei um iphone 11 pro meu namorado. Pqp ...\n",
       "73  nathsfernandes_  n√£o comprei outro celular at√© agora aguardando...\n",
       "74        SpinaBela  Gente do ceu! \\nEu passei a semana mais agunia...\n",
       "75      badgalguiga  ‚ÄúNao me diga q isso √© o iphone 11‚Äù\\n-Nao, comp...\n",
       "76      capitaoluca  Quem dera se o fato de eu ter um iPhone me fiz...\n",
       "77        eulorenaa  Comprei meu iPhone na segunda-feira e minha m√£...\n",
       "78           Quaxlo  fracassado o q  pow comprei um iphone a vista ...\n",
       "79   LoohMartins_19  Meu irm√£o: Comprei o iPhone 7 plus \\nEu: pra m...\n",
       "80         gvsilva7           Comprei um iPhone pra minha princesa üòù‚ò∫Ô∏è\n",
       "81    rafah_avila04  Comprei um iphone dnv ! Desculpa xiaome ! Foi ...\n",
       "82   AlineZdziarski  vcs precisam PARAR de tentar diminuir os outro...\n",
       "83          liipw12  Eu comprei iPhone 5s to  gostando bastante mas...\n",
       "84        EdiCostta  @Tony_Alive1 iPhone 11, comprado em Orlando se...\n",
       "85        amandda_1  Batia o p√© falando que eu s√≥ comprava celular ...\n",
       "86  KarolBarcello12   Comprei o iPhone 8 Plus, mas j√° quero o 11 pro ü§≠\n",
       "87         exoblwck  vai fazer 2 meses q eu comprei um iphone e at√©...\n",
       "88    gabynbarcelos  Mal comprei um cabo Beja deu ruim, pq iPhone ....\n",
       "89   johnnywendel03  Comprei hoje um iPhone pra minha prima.. 11 an...\n",
       "90        luizaa_tk  @Elizaa_coser j√° comprei meu iphone novo amiga...\n",
       "91  guxxtavosantana  Quase comprei um iphone 7, mas no fim peguei o...\n",
       "92          viihs2z                      Comprei um iPhone 11 am√©m üôèüèºü•∞\n",
       "93  clyntonribeiroo                  Comprei um iPhone e n sei mexer üò≠\n",
       "94      JackCrvlh26  Eu me estresso com IPhone n sei pra que compre...\n",
       "95      fofasuxpiro  @thesasar4 entao sara eu vendi meu rim esquerd...\n",
       "96         wellslv_            @focusrare Eu quando comprei meu iphone\n",
       "97     hannaclarahs  comprei um iPhone 7 e n√£o sei usar, continuo u...\n",
       "98    MicheleLeone0                    Comprei 10 capas pro meu iPhone\n",
       "99        danigibin  @Apple comprei meu iPhone 7 que uso atualmente...\n",
       "\n",
       "[100 rows x 2 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Busca por termos\n",
    "status_list = api.GetSearch(term=\"iphone+comprei -filter:retweets\",\n",
    "                            lang='pt',\n",
    "                            count=1000,\n",
    "                            result_type='recent' )\n",
    "\n",
    "\n",
    "# Percorre o Array\n",
    "users_locs = [[tweet.user.screen_name, tweet.text] for tweet in status_list]\n",
    "\n",
    "# Cria um pandas\n",
    "tweet_text = pd.DataFrame(data=users_locs, \n",
    "                    columns=['User', \"Tweet\"])\n",
    "\n",
    "# Printa o Dataframe\n",
    "tweet_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tamanho da base de Treinamento 15\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "alegria    15\n",
       "Name: Sentimento, dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Criar base treinamento \n",
    "base_treinamento=[\n",
    "      (\"este trabalho √© agradavel\",\"alegria\" ),\n",
    "    (\"este trabalho √© agradavel\",\"alegria\" ),\n",
    "    (\"este trabalho √© agradavel\",\"alegria\" ),\n",
    "    (\"este trabalho √© agradavel\",\"alegria\" ),\n",
    "    (\"este trabalho √© agradavel\",\"alegria\" ),\n",
    "    (\"este trabalho √© agradavel\",\"alegria\" ),\n",
    "    (\"este trabalho √© agradavel\",\"alegria\" ),\n",
    "    (\"este trabalho √© agradavel\",\"alegria\" ),\n",
    "    (\"este trabalho √© agradavel\",\"alegria\" ),\n",
    "    (\"este trabalho √© agradavel\",\"alegria\" ),\n",
    "    (\"este trabalho √© agradavel\",\"alegria\" ),\n",
    "    (\"este trabalho √© agradavel\",\"alegria\" ),\n",
    "    (\"este trabalho √© agradavel\",\"alegria\" ),\n",
    "    (\"este trabalho √© agradavel\",\"alegria\" ),\n",
    "    (\"este trabalho √© agradavel\",\"alegria\" )\n",
    "]\n",
    "\n",
    "# Transforma base em pandas\n",
    "exemplo_base=pd.DataFrame(base_treinamento)\n",
    "\n",
    "#D√° nome as colunas\n",
    "exemplo_base.columns=['Frase','Sentimento']\n",
    "\n",
    "# Mostra tamanho da base de treinamento\n",
    "print('Tamanho da base de Treinamento {}'.format(exemplo_base.shape[0]))\n",
    "\n",
    "# Mostra quantidade de cada tipo de sentimento\n",
    "exemplo_base.Sentimento.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alegria    100.0\n",
      "Name: Sentimento, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Percentual de cada tipo de sentimento\n",
    "print((exemplo_base.Sentimento.value_counts()/exemplo_base.shape[0])*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tamanho da base de Treinamento 15\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "alegria    15\n",
       "Name: Sentimento, dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Criar base teste\n",
    "base_teste=[\n",
    "      (\"este trabalho √© agradavel\",\"alegria\" ),\n",
    "    (\"este trabalho √© agradavel\",\"alegria\" ),\n",
    "    (\"este trabalho √© agradavel\",\"alegria\" ),\n",
    "    (\"este trabalho √© agradavel\",\"alegria\" ),\n",
    "    (\"este trabalho √© agradavel\",\"alegria\" ),\n",
    "    (\"este trabalho √© agradavel\",\"alegria\" ),\n",
    "    (\"este trabalho √© agradavel\",\"alegria\" ),\n",
    "    (\"este trabalho √© agradavel\",\"alegria\" ),\n",
    "    (\"este trabalho √© agradavel\",\"alegria\" ),\n",
    "    (\"este trabalho √© agradavel\",\"alegria\" ),\n",
    "    (\"este trabalho √© agradavel\",\"alegria\" ),\n",
    "    (\"este trabalho √© agradavel\",\"alegria\" ),\n",
    "    (\"este trabalho √© agradavel\",\"alegria\" ),\n",
    "    (\"este trabalho √© agradavel\",\"alegria\" ),\n",
    "    (\"este trabalho √© agradavel\",\"alegria\" )\n",
    "]\n",
    "\n",
    "# Transforma base em pandas\n",
    "exemplo_base=pd.DataFrame(base_teste)\n",
    "\n",
    "#D√° nome as colunas\n",
    "exemplo_base.columns=['Frase','Sentimento']\n",
    "\n",
    "# Mostra tamanho da base de treinamento\n",
    "print('Tamanho da base de Treinamento {}'.format(exemplo_base.shape[0]))\n",
    "\n",
    "# Mostra quantidade de cada tipo de sentimento\n",
    "exemplo_base.Sentimento.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['de', 'a', 'o', 'que', 'e', '√©', 'do', 'da', 'em', 'um', 'para',\n",
       "       'com', 'n√£o', 'uma', 'os', 'no', 'se', 'na', 'por', 'mais', 'as',\n",
       "       'dos', 'como', 'mas', 'ao', 'ele', 'das', '√†', 'seu', 'sua', 'ou',\n",
       "       'quando', 'muito', 'nos', 'j√°', 'eu', 'tamb√©m', 's√≥', 'pelo',\n",
       "       'pela', 'at√©', 'isso', 'ela', 'entre', 'depois', 'sem', 'mesmo',\n",
       "       'aos', 'seus', 'quem', 'nas', 'me', 'esse', 'eles', 'voc√™', 'essa',\n",
       "       'num', 'nem', 'suas', 'meu', '√†s', 'minha', 'numa', 'pelos',\n",
       "       'elas', 'qual', 'n√≥s', 'lhe', 'deles', 'essas', 'esses', 'pelas',\n",
       "       'este', 'dele', 'tu', 'te', 'voc√™s', 'vos', 'lhes', 'meus',\n",
       "       'minhas', 'teu', 'tua', 'teus', 'tuas', 'nosso', 'nossa', 'nossos',\n",
       "       'nossas', 'dela', 'delas', 'esta', 'estes', 'estas', 'aquele',\n",
       "       'aquela', 'aqueles', 'aquelas', 'isto', 'aquilo', 'estou', 'est√°',\n",
       "       'estamos', 'est√£o', 'estive', 'esteve', 'estivemos', 'estiveram',\n",
       "       'estava', 'est√°vamos', 'estavam', 'estivera', 'estiv√©ramos',\n",
       "       'esteja', 'estejamos', 'estejam', 'estivesse', 'estiv√©ssemos',\n",
       "       'estivessem', 'estiver', 'estivermos', 'estiverem', 'hei', 'h√°',\n",
       "       'havemos', 'h√£o', 'houve', 'houvemos', 'houveram', 'houvera',\n",
       "       'houv√©ramos', 'haja', 'hajamos', 'hajam', 'houvesse',\n",
       "       'houv√©ssemos', 'houvessem', 'houver', 'houvermos', 'houverem',\n",
       "       'houverei', 'houver√°', 'houveremos', 'houver√£o', 'houveria',\n",
       "       'houver√≠amos', 'houveriam', 'sou', 'somos', 's√£o', 'era', '√©ramos',\n",
       "       'eram', 'fui', 'foi', 'fomos', 'foram', 'fora', 'f√¥ramos', 'seja',\n",
       "       'sejamos', 'sejam', 'fosse', 'f√¥ssemos', 'fossem', 'for', 'formos',\n",
       "       'forem', 'serei', 'ser√°', 'seremos', 'ser√£o', 'seria', 'ser√≠amos',\n",
       "       'seriam', 'tenho', 'tem', 'temos', 't√©m', 'tinha', 't√≠nhamos',\n",
       "       'tinham', 'tive', 'teve', 'tivemos', 'tiveram', 'tivera',\n",
       "       'tiv√©ramos', 'tenha', 'tenhamos', 'tenham', 'tivesse',\n",
       "       'tiv√©ssemos', 'tivessem', 'tiver', 'tivermos', 'tiverem', 'terei',\n",
       "       'ter√°', 'teremos', 'ter√£o', 'teria', 'ter√≠amos', 'teriam'],\n",
       "      dtype='<U12')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#nltk.download('stopwords')\n",
    "lista_Stop=nltk.corpus.stopwords.words('portuguese')\n",
    "np.transpose(lista_Stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fun√ß√£o que retira StopWords\n",
    "def RemoveStopWords(texto):\n",
    "    frases=[]\n",
    "    for (palavras, sentimento) in texto:\n",
    "        #Criamos uma list compreheension para extrair apenas as palavras que n√£o est√£o na list_stop\n",
    "        semStop= [ p for p in palavras.split() if p not in lista_Stop]\n",
    "        #Inserindo as frases com os Labels (sentimento) ja tatadas pela Lista_Stop\n",
    "        frases.append((semStop,sentimento))\n",
    "    return frases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Steemer= t√©cnica de remover sufixos e prefixos de uma palavra,Por exemplo, o stem da palavra cooking √© cook. Um bom algoritmo sabe que ‚Äúing‚Äù √© um sufixo e pode ser removido.\n",
    "#nltk.download('rslp')\n",
    "def aplica_Stemmer(texto):\n",
    "    stemmer=nltk.stem.RSLPStemmer()\n",
    "    #Escolhido o RSLPS pois √© especifico da l√≠ngua portuguesas\n",
    "    frases_sem_Stemming=[]\n",
    "    for (palavras, sentimento) in texto:\n",
    "        com_Stemming=[str(stemmer.stem(p)) for p in palavras.split() if p not in lista_Stop]\n",
    "        frases_sem_Stemming.append((com_Stemming,sentimento))\n",
    "    return frases_sem_Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('este trabalho √© agradavel', 'alegria'),\n",
       " ('este trabalho √© agradavel', 'alegria'),\n",
       " ('este trabalho √© agradavel', 'alegria'),\n",
       " ('este trabalho √© agradavel', 'alegria'),\n",
       " ('este trabalho √© agradavel', 'alegria'),\n",
       " ('este trabalho √© agradavel', 'alegria'),\n",
       " ('este trabalho √© agradavel', 'alegria'),\n",
       " ('este trabalho √© agradavel', 'alegria'),\n",
       " ('este trabalho √© agradavel', 'alegria'),\n",
       " ('este trabalho √© agradavel', 'alegria'),\n",
       " ('este trabalho √© agradavel', 'alegria'),\n",
       " ('este trabalho √© agradavel', 'alegria'),\n",
       " ('este trabalho √© agradavel', 'alegria'),\n",
       " ('este trabalho √© agradavel', 'alegria'),\n",
       " ('este trabalho √© agradavel', 'alegria')]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Aplica o Stemer na Frase\n",
    "frases_com_Stem_treinamento=aplica_Stemmer(base_treinamento)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Frase</th>\n",
       "      <th>Sentimento</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[trabalh, agrada]</td>\n",
       "      <td>alegria</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>[trabalh, agrada]</td>\n",
       "      <td>alegria</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>[trabalh, agrada]</td>\n",
       "      <td>alegria</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>[trabalh, agrada]</td>\n",
       "      <td>alegria</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>[trabalh, agrada]</td>\n",
       "      <td>alegria</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[trabalh, agrada]</td>\n",
       "      <td>alegria</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>[trabalh, agrada]</td>\n",
       "      <td>alegria</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>[trabalh, agrada]</td>\n",
       "      <td>alegria</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>[trabalh, agrada]</td>\n",
       "      <td>alegria</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[trabalh, agrada]</td>\n",
       "      <td>alegria</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                Frase Sentimento\n",
       "1   [trabalh, agrada]    alegria\n",
       "9   [trabalh, agrada]    alegria\n",
       "11  [trabalh, agrada]    alegria\n",
       "12  [trabalh, agrada]    alegria\n",
       "8   [trabalh, agrada]    alegria\n",
       "4   [trabalh, agrada]    alegria\n",
       "10  [trabalh, agrada]    alegria\n",
       "6   [trabalh, agrada]    alegria\n",
       "7   [trabalh, agrada]    alegria\n",
       "5   [trabalh, agrada]    alegria"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Transforma em Dataframa a base de Stemer\n",
    "pd.DataFrame(frases_com_Stem_treinamento, columns=['Frase','Sentimento']).sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aplica Stem na base de teste\n",
    "frases_com_Stem_teste=aplica_Stemmer(base_teste)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cria fun√ß√£o para retornar as palavras da frase, sem a classifica√ß√£o(sentimento)\n",
    "def busca_Palavras(frases):\n",
    "    todas_Palavras=[]\n",
    "    for (palavras,sentimento) in frases:\n",
    "        todas_Palavras.extend(palavras)\n",
    "    return todas_Palavras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aplica fun√ß√£o acima em cada base\n",
    "palavras_treinamento=busca_Palavras(frases_com_Stem_treinamento)\n",
    "palavras_teste=busca_Palavras(frases_com_Stem_teste)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quantidade de palvras 0    30\n",
      "dtype: int64 \n"
     ]
    }
   ],
   "source": [
    "# Mostra quantidade de palavras na base\n",
    "print (\"Quantidade de palvras {} \".format(pd.DataFrame(palavras_treinamento).count()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fun√ß√£o para verificar a quantidade de vezes que a palavras √© mencionada\n",
    "def busca_frequencia(palavras):\n",
    "    palavras=nltk.FreqDist(palavras)\n",
    "    return palavras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chama fun√ß√£o acima\n",
    "frequencia_treinamento=busca_frequencia(palavras_treinamento)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('trabalh', 15), ('agrada', 15)]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# A fun√ß√£o most_common do NLTK possibilita visualizar quais as palavras que ocorrem com maior frequ√™ncia em nosso texto.\n",
    "frequencia_treinamento.most_common(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Executa fun√ß√£o parecida com a mostcommon\n",
    "frequencia_teste=busca_frequencia(palavras_teste)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fun√ß√£o para retornar somente as palavras unicas\n",
    "def busca_palavras_unicas(frequencia):\n",
    "    freq=frequencia.keys()\n",
    "    return freq\n",
    "\n",
    "# Chama fun√ß√£o acima\n",
    "palavras_unicas_treinamento=busca_palavras_unicas(frequencia_treinamento)\n",
    "palavras_unicas_teste=busca_palavras_unicas(frequencia_teste)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cria fun√ß√£o para identificar quais as palavras √∫nicas est√£o no docuemnto passado para a fun√ß√£o\n",
    "def extrator_palavras(documento):\n",
    "        #Utilizado set() para associar a vari√°vel doc.com o parametro que esta chegando\n",
    "        doc=set(documento)\n",
    "        caracteristicas={}\n",
    "        for palavras in palavras_unicas_treinamento:\n",
    "            caracteristicas['%s' % palavras]=(palavras in doc)\n",
    "        return caracteristicas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# devido a necessidade de aplcia√ß√£o da fun√ß√£o Extrato_Palavras sobre as bases de Treinamento e Teste, como a vari√°vel palavras_unicas_teste exige aplica√ß√£o isolada, precisamos criar uma fun√ß√£o apartada somente\n",
    "# para a base de teste\n",
    "def extrator_palavras_teste(documento):\n",
    "    doc=set(documento)\n",
    "    caracteristicas={}\n",
    "    for palavras in palavras_unicas_teste:\n",
    "        caracteristicas['%s' % palavras]=(palavras in doc)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_completa_treinamento=nltk.classify.apply_features(extrator_palavras, frases_com_Stem_treinamento)\n",
    "base_completa_teste=nltk.classify.apply_features(extrator_palavras_teste,frases_com_Stem_teste)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['alegria']\n"
     ]
    }
   ],
   "source": [
    "# O algoritmo NaiveBayes monta a tabela de probabilidade\n",
    "classificador=nltk.NaiveBayesClassifier.train(base_completa_treinamento)\n",
    "\n",
    "print(classificador.labels())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Informative Features\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(classificador.show_most_informative_features(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>None</td>\n",
       "      <td>alegria</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>None</td>\n",
       "      <td>alegria</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>None</td>\n",
       "      <td>alegria</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>None</td>\n",
       "      <td>alegria</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>None</td>\n",
       "      <td>alegria</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>None</td>\n",
       "      <td>alegria</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>None</td>\n",
       "      <td>alegria</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>None</td>\n",
       "      <td>alegria</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>None</td>\n",
       "      <td>alegria</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>None</td>\n",
       "      <td>alegria</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>None</td>\n",
       "      <td>alegria</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>None</td>\n",
       "      <td>alegria</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>None</td>\n",
       "      <td>alegria</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>None</td>\n",
       "      <td>alegria</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>None</td>\n",
       "      <td>alegria</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       0        1\n",
       "0   None  alegria\n",
       "1   None  alegria\n",
       "2   None  alegria\n",
       "3   None  alegria\n",
       "4   None  alegria\n",
       "5   None  alegria\n",
       "6   None  alegria\n",
       "7   None  alegria\n",
       "8   None  alegria\n",
       "9   None  alegria\n",
       "10  None  alegria\n",
       "11  None  alegria\n",
       "12  None  alegria\n",
       "13  None  alegria\n",
       "14  None  alegria"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Verifica a acuracia do modelo\n",
    "pd_base_completa=pd.DataFrame(base_completa_teste)\n",
    "\n",
    "#print(nltk.classify.accuracy(classificador,pd_base_completa))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'copy'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-49-26219f84e62c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[1;31m#print(frase)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[1;31m#print(classe)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m     \u001b[0mresultado\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mclassificador\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclassify\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfrase\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mresultado\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mclasse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m         \u001b[0merros\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclasse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mresultado\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mfrase\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\nltk\\classify\\naivebayes.py\u001b[0m in \u001b[0;36mclassify\u001b[1;34m(self, featureset)\u001b[0m\n\u001b[0;32m     88\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     89\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclassify\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeatureset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 90\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprob_classify\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeatureset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     91\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     92\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mprob_classify\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeatureset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\nltk\\classify\\naivebayes.py\u001b[0m in \u001b[0;36mprob_classify\u001b[1;34m(self, featureset)\u001b[0m\n\u001b[0;32m     94\u001b[0m         \u001b[1;31m# Otherwise, we'll just assign a probability of 0 to\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     95\u001b[0m         \u001b[1;31m# everything.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 96\u001b[1;33m         \u001b[0mfeatureset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfeatureset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     97\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mfname\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeatureset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     98\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mlabel\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_labels\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'copy'"
     ]
    }
   ],
   "source": [
    "# Agrega linhas com erros de classifica√ß√£o\n",
    "erros=[]\n",
    "for (frase,classe) in base_completa_teste:\n",
    "    #print(frase)\n",
    "    #print(classe)\n",
    "    resultado=classificador.classify(frase)\n",
    "    if resultado != classe:\n",
    "        erros.append((classe,resultado,frase))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'copy'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-50-27f19c7d9902>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mprevisto\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mfrase\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mclasse\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mbase_completa_teste\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m     \u001b[0mresultado\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mclassificador\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclassify\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfrase\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m     \u001b[0mprevisto\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresultado\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[0mesperando\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclasse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\nltk\\classify\\naivebayes.py\u001b[0m in \u001b[0;36mclassify\u001b[1;34m(self, featureset)\u001b[0m\n\u001b[0;32m     88\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     89\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclassify\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeatureset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 90\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprob_classify\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeatureset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     91\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     92\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mprob_classify\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeatureset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\nltk\\classify\\naivebayes.py\u001b[0m in \u001b[0;36mprob_classify\u001b[1;34m(self, featureset)\u001b[0m\n\u001b[0;32m     94\u001b[0m         \u001b[1;31m# Otherwise, we'll just assign a probability of 0 to\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     95\u001b[0m         \u001b[1;31m# everything.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 96\u001b[1;33m         \u001b[0mfeatureset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfeatureset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     97\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mfname\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeatureset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     98\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mlabel\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_labels\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'copy'"
     ]
    }
   ],
   "source": [
    "# Cria matriz de confus√£o\n",
    "from nltk.metrics import ConfusionMatrix\n",
    "esperado=[]\n",
    "previsto=[]\n",
    "for (frase,classe) in base_completa_teste:\n",
    "    resultado=classificador.classify(frase)\n",
    "    previsto.append(resultado)\n",
    "    esperando.append(classe)\n",
    "\n",
    "matriz=ConfusionMatrix(esperado,previsto)\n",
    "print (matriz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alegria: 1.000000\n"
     ]
    }
   ],
   "source": [
    "teste=\"Caramba que transito!\"\n",
    "testeStemming=[]\n",
    "stemmer=nltk.stem.RSLPStemmer()\n",
    "for (palavras_treinamento) in teste.split():\n",
    "    comStem=[p for p in palavras_treinamento.split()]\n",
    "    testeStemming.append(str(stemmer.stem(comStem[0])))\n",
    "\n",
    "novo=extrator_palavras(testeStemming)\n",
    "#print(classificador.classify(novo))\n",
    "distribuicao = classificador.prob_classify(novo)\n",
    "for classe in distribuicao.samples():\n",
    "    print('%s: %f' % (classe, distribuicao.prob(classe)))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "LendoTwitter.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
